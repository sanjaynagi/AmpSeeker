# The main entry point of your workflow.
# After configuring, running snakemake -n in a clone of this repository should successfully execute a dry-run of the workflow.
report: "report/workflow.rst"
import pandas as pd 
import numpy as np

configfile:"config/config.yaml"

metadata = pd.read_csv(config['metadata'], sep="\t")
samples = metadata['sampleID']
# Specify below whether sequence data provided is amplicon or whole genome sequence data.
sequence_data = "amplicon" 

if sequence_data == "amplicon":
	REF=config['ref']['amplicon']
	BED=config['bed']['amplicon']
	coverage = expand("results/coverage/{sample}.per-base.bed.gz", sample=samples, ref=REF)

if sequence_data == "wholegenome":
	REF=config['ref']['wholegenome']
	BED=config['bed']['wholegenome']
	coverage = expand("results/wholegenome/coverage/windowed/{sample}.regions.bed.gz", sample=samples)
#present = pd.read_csv(config['present_samples'], sep="\t", header=None)

# We have to remove absent samples, or workflow will fail 
#samples = metadata[metadata['sampleID'].isin(present[0])]['sampleID']

# Split into two sample sets as bcftools merge cant take over 1020 files
# So we must do two rounds of merging
if len(metadata) > 1020:
	n_samples = len(metadata)
	half = int(n_samples/2)
	samples1 = metadata['sampleID'][:half]
	samples2 = metadata['sampleID'][half:]
	#samples = samples[~np.isin(samples, ["AgamDaoLSTM1_1375", "AgamDaoLSTM1_1378", "])]
	include: "rules/BgzipTabixMerge.smk"
else:
	include: "rules/BgzipTabixLite.smk"
	
rule all:
	input:
		# The first rule should define the default target files
		coverage,
		stats = expand("results/alignments/bamStats/{sample}.flagstat", sample=samples,ref=REF),
		vcf = expand("results/vcfs/AgamDaoLSTM_merged.vcf", ref=REF),
		#trimmed = expand("resources/reads/trimmed/{sample}_{n}.fastq.gz", n=[1,2], sample=samples, ref=REF)

include: "rules/qc.smk"
include: "rules/AlignmentVariantCalling.smk"


