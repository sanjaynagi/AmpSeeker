# The main entry point of your workflow.
# After configuring, running snakemake -n in a clone of this repository should successfully execute a dry-run of the workflow.
report: "report/workflow.rst"
import pandas as pd 
import numpy as np

configfile:"config/config.yaml"

metadata = pd.read_csv(config['metadata'], sep="\t")
samples = metadata['sampleID']
# Specify below whether sequence data provided is amplicon or whole genome sequence data.
sequence_data = "amplicon" 

if sequence_data == "amplicon":
	REF=config['ref']['amplicon']
	BED=config['bed']['amplicon']
	coverage = expand("results/coverage/{sample}.per-base.bed.gz", sample=samples, ref=REF)

if sequence_data == "wholegenome":
	REF=config['ref']['wholegenome']
	BED=config['bed']['wholegenome']
	coverage = expand("results/wholegenome/coverage/windowed/{sample}.regions.bed.gz", sample=samples)

# Split into two sample sets as bcftools merge cant take over 1000 files
# So we must do two rounds of merging
if len(metadata) > 1000:
	n_samples = len(metadata)
	half = int(n_samples/2)
	samples1 = metadata['sampleID'][:half]
	samples2 = metadata['sampleID'][half:]
	large_sample_size = True
else:
	large_sample_size = False
	samples1 = []
	samples2 = []

	
include: "rules/BgzipTabixMerge.smk"
include: "rules/qc.smk"
include: "rules/AlignmentVariantCalling.smk"

rule all:
	input:
		# The first rule should define the default target files
		coverage,
		stats = expand("results/alignments/bamStats/{sample}.flagstat", sample=samples,ref=REF),
		vcf = expand("results/vcfs/{dataset}_LSTM_merged.vcf", ref=REF, dataset=config['dataset']),
		merge_vcfs = "results/vcfs/.complete.merge_vcfs" if large_sample_size else [],
