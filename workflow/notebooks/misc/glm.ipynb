{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code - get genotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import malariagen_data\n",
    "import numpy as np\n",
    "import allel\n",
    "\n",
    "def vcf_to_siaya_glm_data(vcf_path, df_samples, max_missing_filter=0.20, min_maf_filter=0.02 ,split_multiallelic=True, convert_genotypes=True):\n",
    "    # Read VCF and create a dictionary\n",
    "    vcf_df = vcf_to_siaya_df(vcf_path, df_samples, max_missing_filter=max_missing_filter, min_maf_filter=min_maf_filter)\n",
    "    samples = vcf_df.columns[6:]\n",
    "\n",
    "    if split_multiallelic:\n",
    "        vcf_df = split_rows_with_multiple_alleles(vcf_df, samples)\n",
    "\n",
    "    # Convert genotypes to 0/1/2\n",
    "    if convert_genotypes:\n",
    "        vcf_df = convert_genotype_to_alt_allele_count(vcf_df, samples)\n",
    "\n",
    "    vcf_df = vcf_df.assign(snp_id=lambda x: \"snp_\" + x.CHROM + \":\" + x.POS.astype(str) + \"_\" + x.ALT)\n",
    "    vcf_df = vcf_df.set_index('snp_id')\n",
    "    vcf_df = vcf_df.drop(columns=['CHROM', 'POS', 'FILTER_PASS', 'REF', 'ALT', 'ANN']).T\n",
    "    vcf_df = pd.concat([df_samples.set_index('sample_id').query('location == \"Siaya\"'), vcf_df], axis=1)\n",
    "    \n",
    "    return vcf_df\n",
    "    \n",
    "\n",
    "def vcf_to_siaya_df(vcf_path, df_samples, max_missing_filter=0.20, min_maf_filter=0.02, query=None):\n",
    "    # Read VCF file\n",
    "    vcf_dict = allel.read_vcf(vcf_path, fields='*')\n",
    "    samples = vcf_dict['samples']\n",
    "    contig = vcf_dict['variants/CHROM'] \n",
    "    pos = vcf_dict['variants/POS']\n",
    "    filter_pass = vcf_dict['variants/FILTER_PASS']\n",
    "    ref = vcf_dict['variants/REF']\n",
    "    alt = np.apply_along_axis(lambda x: ','.join(x[x != '']), 1, vcf_dict['variants/ALT'])\n",
    "    ann = vcf_dict['variants/ANN']\n",
    "    geno = allel.GenotypeArray(vcf_dict['calldata/GT'])\n",
    "    \n",
    "    print(f\"Initial number of samples: {len(samples)}\")\n",
    "    print(f\"Initial number of SNPs: {geno.shape[0]}\")\n",
    "    \n",
    "    # filter to Siaya data\n",
    "    siaya_mask = df_samples.eval(\"sample_id.str.contains('Siaya')\", engine='python')\n",
    "    sample_mask = np.isin(samples, df_samples[siaya_mask].sample_id)\n",
    "\n",
    "    geno = geno.compress(sample_mask, axis=1)\n",
    "    samples = samples[sample_mask]\n",
    "\n",
    "    if query is not None:\n",
    "        mask = df_samples.eval(query, engine='python')\n",
    "        geno = geno.compress(mask, axis=1)\n",
    "        samples = samples[mask]\n",
    "    \n",
    "    print(f\"Final number of samples: {len(samples)}\")\n",
    "        \n",
    "    # Calculate allele counts and apply filters\n",
    "    ac = geno.count_alleles(max_allele=3)\n",
    "    seg_mask = ac.is_segregating()\n",
    "    miss_mask = (geno.is_missing().sum(axis=1) <= int(geno.shape[1]*max_missing_filter))\n",
    "    maf_mask = ac.to_frequencies()[:, 1:].sum(axis=1) > min_maf_filter\n",
    "    \n",
    "    print(f\"invariant SNPs removed: {np.sum(~seg_mask)}\")\n",
    "    print(f\"SNPs removed due to high missingness (>{max_missing_filter*100}%): {np.sum(~miss_mask)}\")\n",
    "    print(f\"SNPs removed due to low MAF (<{min_maf_filter}): {np.sum(~maf_mask)}\")\n",
    "    \n",
    "    # Apply all filters\n",
    "    mask = np.logical_and(seg_mask, np.logical_and(miss_mask, maf_mask))\n",
    "    \n",
    "    # Compress data based on mask\n",
    "    geno = geno.compress(mask, axis=0)\n",
    "    contig = contig[mask]\n",
    "    pos = pos[mask]\n",
    "    filter_pass = filter_pass[mask]\n",
    "    ref = ref[mask]\n",
    "    alt = alt[mask]\n",
    "    ann = ann[mask]\n",
    "    \n",
    "    print(f\"Final number of SNPs after all filters: {geno.shape[0]}\")\n",
    "    \n",
    "    # Create DataFrame versions of VCF and genotypes\n",
    "    vcf_df = pd.DataFrame({'CHROM': contig, 'POS': pos, 'FILTER_PASS': filter_pass, 'REF': ref, 'ALT': alt, 'ANN': ann})\n",
    "    geno_df = pd.DataFrame(geno.to_gt().astype(str), columns=samples)\n",
    "    \n",
    "    # Combine VCF and genotype DataFrames\n",
    "    vcf = pd.concat([vcf_df, geno_df], axis=1)\n",
    "    \n",
    "    print(f\"Final DataFrame shape: {vcf.shape}\")\n",
    "    \n",
    "    return vcf\n",
    "\n",
    "def split_rows_with_multiple_alleles(df, samples):\n",
    "    # Create an empty list to store the new rows\n",
    "    new_rows = []\n",
    "    # Iterate through each row\n",
    "    for index, row in df.iterrows():\n",
    "        alt_alleles = row['ALT'].split(',')\n",
    "        # Check if there are multiple alleles in the ALT field\n",
    "        if len(alt_alleles) > 1:\n",
    "            for allele_num, allele in enumerate(alt_alleles):\n",
    "                # Create a new row for each allele\n",
    "                new_row = row.copy()\n",
    "                new_row['ALT'] = allele\n",
    "                # Update genotype fields\n",
    "                for col in samples:\n",
    "                    genotype = row[col]\n",
    "                    # Split the genotype and process it\n",
    "                    if genotype != './.':\n",
    "                        gt_alleles = genotype.split('/')\n",
    "                        new_gt = ['0' if (int(gt) != allele_num + 1 and gt != '0') else gt for gt in gt_alleles]\n",
    "                        new_row[col] = '/'.join(new_gt)\n",
    "                new_rows.append(new_row)\n",
    "        else:\n",
    "            new_rows.append(row)\n",
    "    \n",
    "    new_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def convert_genotype_to_alt_allele_count(df, samples):\n",
    "    nalt_df = df.copy()\n",
    "    # Iterate through each row\n",
    "    for index, row in df.iterrows():\n",
    "        # Update genotype fields\n",
    "        for col in samples:\n",
    "                genotype = row[col]\n",
    "                if genotype != './.':\n",
    "                    # Split the genotype and count non-zero alleles\n",
    "                    alleles = genotype.split('/')\n",
    "                    alt_allele_count = sum([1 for allele in alleles if allele != '0'])\n",
    "                    nalt_df.at[index, col] = alt_allele_count\n",
    "                else:\n",
    "                    nalt_df.at[index, col] = np.nan\n",
    "\n",
    "    return nalt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code -  run GLM and process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pseudo_r2(results):\n",
    "    \"\"\"\n",
    "    Calculate different pseudo R² measures for a fitted GLM model\n",
    "    \n",
    "    Parameters:\n",
    "    results : statsmodels GLMResults object\n",
    "        The fitted model results\n",
    "        \n",
    "    Returns:\n",
    "    dict : Dictionary containing different pseudo R² measures\n",
    "    \"\"\"\n",
    "    ll_null = results.null_deviance / -2\n",
    "    ll_model = results.deviance / -2\n",
    "    n = results.nobs\n",
    "    \n",
    "    r2_mcfadden = 1 - (ll_model / ll_null)\n",
    "    r2_coxsnell = 1 - np.exp((2/n) * (ll_null - ll_model))\n",
    "    r2_nagelkerke = r2_coxsnell / (1 - np.exp((2/n) * ll_null))\n",
    "    \n",
    "    return {\n",
    "        'McFadden R²': r2_mcfadden,\n",
    "        'Nagelkerke R²': r2_nagelkerke\n",
    "    }\n",
    "\n",
    "def glm_all_snps(snp_df):\n",
    "    import statsmodels.formula.api as smf\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.multitest import fdrcorrection\n",
    "    \n",
    "    dfs = []\n",
    "    for snp in snp_df.filter(like=\"snp_\").columns:\n",
    "        glm_data = snp_df[['phenotype', snp]].dropna()\n",
    "        glm_data[snp] = glm_data[snp].astype(int)\n",
    "        glm_data.columns = ['phenotype', 'snp']\n",
    "    \n",
    "        df_contingency = pd.crosstab(glm_data['phenotype'], glm_data['snp'])\n",
    "        \n",
    "        formula = f'phenotype ~ snp'\n",
    "        logit_model = smf.glm(formula=formula, data=glm_data, family=sm.families.Binomial())\n",
    "        log_results = logit_model.fit()\n",
    "        \n",
    "        # Calculate pseudo R² measures\n",
    "        r2_values = calculate_pseudo_r2(log_results)\n",
    "    \n",
    "        pval_data = results_summary_to_dataframe(log_results)\n",
    "        # Add R² values to the results\n",
    "        for r2_name, r2_value in r2_values.items():\n",
    "            pval_data[r2_name] = r2_value\n",
    "            \n",
    "        dfs.append(pval_data.assign(snp=snp))\n",
    "        \n",
    "    df_eff = pd.concat(dfs).query(\"index != 'Intercept'\")\n",
    "    res = fdrcorrection(df_eff['pvals'], alpha=0.05)\n",
    "    df_eff['fdr'] = res[1]\n",
    "    df_eff['fdr_sig'] = res[0]\n",
    "    return process_effect_sizes(df_eff)\n",
    "\n",
    "\n",
    "def process_effect_sizes(df_eff):\n",
    "    df_eff = df_eff.assign(CHROM=lambda x: x.snp.str.split(\":\").str[0].str.replace(\"snp_\", \"\"), \n",
    "                   POS_ALT=lambda x: x.snp.str.split(\":\").str[1]).assign(POS=lambda x: x.POS_ALT.str.split(\"_\").str[0].astype(int))\n",
    "    df_eff = df_eff.merge(snp_data[['CHROM', 'POS', 'ANN']])\n",
    "    cols = ['snp'] + [col for col in df_eff.columns if col != 'snp']\n",
    "    df_eff = df_eff[cols].drop(columns=['CHROM', 'POS'])\n",
    "    df_eff = df_eff.set_index('snp')\n",
    "    return df_eff    \n",
    "\n",
    "\n",
    "def results_summary_to_dataframe(results):\n",
    "    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n",
    "    pvals = results.pvalues\n",
    "    coeff = results.params\n",
    "    conf_lower = results.conf_int()[0]\n",
    "    conf_higher = results.conf_int()[1]\n",
    "\n",
    "    results_df = pd.DataFrame({\"pvals\":pvals,\n",
    "                               \"odds_ratio\":np.exp(coeff),\n",
    "                               \"conf_lower\":np.exp(conf_lower),\n",
    "                               \"conf_higher\":np.exp(conf_higher)\n",
    "                                })\n",
    "    \n",
    "    results_df.loc[:, 'sig'] = [True if pval <= 0.05 else False for pval in pvals]\n",
    "    results_df = results_df[[\"odds_ratio\",\"pvals\",\"conf_lower\",\"conf_higher\", \"sig\"]]\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code - compute correlations and select snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_snp_correlations(df, correlation_threshold=0.5):\n",
    "    corr_matrix = df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "   \n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > correlation_threshold:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    \n",
    "    print(f\"Total SNPs analyzed: {len(df.columns)}\")\n",
    "    print(f\"Number of highly correlated SNP pairs (|r| > {correlation_threshold}): {len(high_corr_pairs)}\")\n",
    "    \n",
    "    return corr_matrix, high_corr_pairs\n",
    "\n",
    "def plot_correlation_heatmap(corr_matrix):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('SNP Correlation Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "def select_representative_snps(df, high_corr_pairs):\n",
    "    to_remove = set()\n",
    "    pos_corr_removed = 0\n",
    "    neg_corr_removed = 0\n",
    "    \n",
    "    for snp1, snp2, corr in high_corr_pairs:\n",
    "        if snp1 not in to_remove and snp2 not in to_remove:\n",
    "            if corr < 0:  # Negative correlation\n",
    "                if df.loc[snp1, 'odds_ratio'] < 1:\n",
    "                    to_remove.add(snp1)\n",
    "                    neg_corr_removed += 1\n",
    "                elif df.loc[snp2, 'odds_ratio'] < 1:\n",
    "                    to_remove.add(snp2)\n",
    "                    neg_corr_removed += 1\n",
    "                else:\n",
    "                    if df.loc[snp1, 'fdr'] < df.loc[snp2, 'fdr']:\n",
    "                        to_remove.add(snp2)\n",
    "                        neg_corr_removed += 1\n",
    "                    else:\n",
    "                        to_remove.add(snp1)\n",
    "                        neg_corr_removed += 1\n",
    "            else:  # Positive correlation\n",
    "                if df.loc[snp1, 'fdr'] < df.loc[snp2, 'fdr']:\n",
    "                    to_remove.add(snp2)\n",
    "                    pos_corr_removed += 1\n",
    "                else:\n",
    "                    to_remove.add(snp1)\n",
    "                    pos_corr_removed += 1\n",
    "    \n",
    "    selected_snps = np.array(list(set(df.index) - to_remove))\n",
    "    \n",
    "    print(f\"Total SNPs before selection: {len(df)}\")\n",
    "    print(f\"SNPs removed due to positive correlation: {pos_corr_removed}\")\n",
    "    print(f\"SNPs removed due to negative correlation: {neg_corr_removed}\")\n",
    "    print(f\"Total SNPs removed: {len(to_remove)}\")\n",
    "    print(f\"SNPs retained after selection: {len(selected_snps)}\")\n",
    "    \n",
    "    return selected_snps\n",
    "\n",
    "def linked(df_genos, df_eff, correlation_threshold=0.5):\n",
    "    corr_matrix, high_corr_pairs = compute_snp_correlations(df_genos.filter(like='snp_'), correlation_threshold)\n",
    "    plot_correlation_heatmap(corr_matrix)\n",
    "    selected_snps = select_representative_snps(df_eff, high_corr_pairs)\n",
    "    return np.sort(selected_snps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code - plot PRS scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "def plot_snp_frequency(df_snp, selected_snps):\n",
    "    df = df_snp.loc[:, selected_snps]\n",
    "    \n",
    "    contigs = df.columns.str.split(\"_\").str.get(1).str.slice(0,2)\n",
    "    total_alleles = df.shape[0]\n",
    "    freq = (df.sum() / (total_alleles * 2)).to_frame().assign(contig=contigs.to_list()).reset_index()\n",
    "    freq.columns = ['snp_id', 'freq', 'contig']\n",
    "    fig = px.bar(freq, x='snp_id', y='freq', color='contig', template='simple_white', width=800, height=300)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrating polygenic risk scores into the LLINEUP ento and epi models\n",
    "First lets load the metadata for the Siaya amplicon-sequenced samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amp_samples = pd.read_csv(\"/home/sanj/Projects/amplicon-seq/ampseq-agvampir002/results/config/metadata.qcpass.tsv\", sep=\"\\t\")\n",
    "df_amp_samples = df_amp_samples.assign(phenotype=df_amp_samples.sample_id.str.extract(r'_(Dead|Alive)_'))\n",
    "df_amp_samples = df_amp_samples.rename(columns={'sampleID':'sample_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform association tests with separate binomial GLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data = pd.read_excel(\"../resources/ampseq-vigg002-snps-amplicons.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"pandas.core.arraylike\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"allel.io\")\n",
    "\n",
    "df_genos = {}\n",
    "df_effs = {}\n",
    "methods = ['targets', 'amplicons']\n",
    "methods  =  ['amplicons']\n",
    "for method in methods:\n",
    "    df_genos[method] = vcf_to_siaya_glm_data(\n",
    "        vcf_path=f\"../resources/ampseq-vigg002.annot.{method}.vcf\", \n",
    "        df_samples=df_amp_samples, \n",
    "        min_maf_filter=0.02, \n",
    "        max_missing_filter=0.20\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # run a glm on each snp on the input data \n",
    "    df_effs[method] = glm_all_snps(df_genos[method])\n",
    "    # df_effs[method].to_csv(f\"glm-siaya-effect-size-{method}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genos['amplicons'].query(\"taxon  != 'coluzzii'\").to_csv(\"../df_geno_amplicons.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute correlations between snps and prune snps in LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_snps = {}\n",
    "for method in methods:\n",
    "    selected_snps[method] = linked(df_genos[method].filter(like='snp_').dropna().astype(int), df_effs[method])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in methods:\n",
    "    print(method)\n",
    "    plot_snp_frequency(df_genos[method], selected_snps=selected_snps[method])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
