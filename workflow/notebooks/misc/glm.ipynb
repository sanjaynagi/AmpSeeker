{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association study "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import allel\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def _print_filter_stats(stage_label, total_snps, miss_mask, maf_mask, max_missing_filter, min_maf_filter, extra_removed=None):\n",
    "    \"\"\"Consistent SNP filtering report for any stage.\"\"\"\n",
    "    final_mask = miss_mask & maf_mask\n",
    "    removed_missing = int((~miss_mask).sum())\n",
    "    removed_maf = int((~maf_mask).sum())\n",
    "    removed_overall = int((~final_mask).sum())\n",
    "    max_missing_pct = max_missing_filter * 100\n",
    "    min_maf_threshold = min_maf_filter\n",
    "\n",
    "    print(f\"SNP filtering stats ({stage_label}):\")\n",
    "    print(f\"Total SNPs evaluated: {int(total_snps)}\")\n",
    "    if extra_removed is not None:\n",
    "        print(f\"Removed before MAF/missingness (non-segregating): {int(extra_removed)}\")\n",
    "    print(f\"Removed for missingness (>{max_missing_pct}%): {removed_missing}\")\n",
    "    print(f\"Removed for low MAF (<={min_maf_threshold}): {removed_maf}\")\n",
    "    print(f\"Removed overall (failed either filter): {removed_overall}\")\n",
    "    print(f\"SNPs retained after filtering: {int(final_mask.sum())}\")\n",
    "\n",
    "\n",
    "def vcf_to_glm_data(vcf_path, df_samples, sample_query='sample_id.str.contains(\"Siaya\")', max_missing_filter=0.20, min_maf_filter=0.02, split_multiallelic=True, convert_genotypes=True):\n",
    "    \"\"\"\n",
    "    Process a VCF file and prepare genotype data for GLM analysis.\n",
    "\n",
    "    Parameters:\n",
    "    vcf_path : str\n",
    "        Path to the VCF file.\n",
    "    df_samples : pd.DataFrame\n",
    "        DataFrame containing sample metadata, including sample IDs and locations.\n",
    "    max_missing_filter : float, optional\n",
    "        Maximum allowed proportion of missing genotypes per SNP (default is 0.20).\n",
    "    min_maf_filter : float, optional\n",
    "        Minimum minor allele frequency (MAF) threshold for filtering SNPs (default is 0.02).\n",
    "    split_multiallelic : bool, optional\n",
    "        Whether to split multiallelic SNPs into separate rows (default is True).\n",
    "    convert_genotypes : bool, optional\n",
    "        Whether to convert genotypes to alternate allele counts (0, 1, 2) (default is True).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A DataFrame containing processed genotype data, with SNPs as columns and samples as rows.\n",
    "    \"\"\"\n",
    "    vcf_df = vcf_to_df(\n",
    "        vcf_path,\n",
    "        df_samples,\n",
    "        sample_query=sample_query,\n",
    "        max_missing_filter=max_missing_filter,\n",
    "        min_maf_filter=min_maf_filter,\n",
    "    )\n",
    "    samples = vcf_df.columns[6:]\n",
    "\n",
    "    if split_multiallelic:\n",
    "        vcf_df = split_rows_with_multiple_alleles(vcf_df, samples)\n",
    "\n",
    "    if convert_genotypes:\n",
    "        vcf_df = convert_genotype_to_alt_allele_count(vcf_df, samples)\n",
    "\n",
    "    vcf_df = vcf_df.assign(\n",
    "        snp_id=lambda x: \"snp_\" + x.CHROM.astype(str) + \":\" + x.POS.astype(str) + \"_\" + x.REF.astype(str) + \">\" + x.ALT.astype(str)\n",
    "    )\n",
    "\n",
    "    vcf_df = apply_final_snp_filters(\n",
    "        vcf_df,\n",
    "        samples=samples,\n",
    "        max_missing_filter=max_missing_filter,\n",
    "        min_maf_filter=min_maf_filter,\n",
    "        genotypes_are_alt_counts=convert_genotypes,\n",
    "    )\n",
    "\n",
    "    vcf_df = vcf_df.set_index('snp_id')\n",
    "    vcf_df = vcf_df.drop(columns=['CHROM', 'POS', 'FILTER_PASS', 'REF', 'ALT', 'ANN']).T\n",
    "    vcf_df = pd.concat([df_samples.set_index('sample_id').query(sample_query, engine='python'), vcf_df], axis=1)\n",
    "\n",
    "    return vcf_df\n",
    "\n",
    "\n",
    "def vcf_to_df(vcf_path, df_samples, sample_query, max_missing_filter=0.20, min_maf_filter=0.02, query2=None):\n",
    "    \"\"\"Read VCF, subset samples, and apply initial SNP filtering for performance.\"\"\"\n",
    "    vcf_dict = allel.read_vcf(vcf_path, fields='*')\n",
    "    samples = vcf_dict['samples']\n",
    "    contig = vcf_dict['variants/CHROM']\n",
    "    pos = vcf_dict['variants/POS']\n",
    "    filter_pass = vcf_dict['variants/FILTER_PASS']\n",
    "    ref = vcf_dict['variants/REF']\n",
    "    alt = [','.join([a for a in row if a != '']) for row in vcf_dict['variants/ALT']]\n",
    "    alt = np.array(alt, dtype=object)\n",
    "    ann = vcf_dict['variants/ANN']\n",
    "    geno = allel.GenotypeArray(vcf_dict['calldata/GT'])\n",
    "\n",
    "    print(f\"Initial number of samples: {len(samples)}\")\n",
    "    print(f\"Initial number of SNPs: {geno.shape[0]}\")\n",
    "\n",
    "    mask = df_samples.eval(sample_query, engine='python')\n",
    "    sample_mask = np.isin(samples, df_samples[mask].sample_id)\n",
    "\n",
    "    geno = geno.compress(sample_mask, axis=1)\n",
    "    samples = samples[sample_mask]\n",
    "\n",
    "    if query2 is not None:\n",
    "        mask = df_samples.eval(query2, engine='python')\n",
    "        geno = geno.compress(mask, axis=1)\n",
    "        samples = samples[mask]\n",
    "\n",
    "    print(f\"Final number of samples after sample filtering: {len(samples)}\")\n",
    "\n",
    "    ac = geno.count_alleles(max_allele=3)\n",
    "    seg_mask = ac.is_segregating()\n",
    "\n",
    "    geno_seg = geno.compress(seg_mask, axis=0)\n",
    "    ac_seg = geno_seg.count_alleles(max_allele=3)\n",
    "\n",
    "    miss_mask = geno_seg.is_missing().mean(axis=1) <= max_missing_filter\n",
    "    maf = np.minimum(ac_seg.to_frequencies()[:, 1:].sum(axis=1), 1 - ac_seg.to_frequencies()[:, 1:].sum(axis=1))\n",
    "    maf_mask = maf > min_maf_filter\n",
    "\n",
    "    _print_filter_stats(\n",
    "        stage_label='initial pre-split',\n",
    "        total_snps=len(geno_seg),\n",
    "        miss_mask=miss_mask,\n",
    "        maf_mask=maf_mask,\n",
    "        max_missing_filter=max_missing_filter,\n",
    "        min_maf_filter=min_maf_filter,\n",
    "        extra_removed=int((~seg_mask).sum()),\n",
    "    )\n",
    "\n",
    "    keep_mask_seg = miss_mask & maf_mask\n",
    "\n",
    "    geno_final = geno_seg.compress(keep_mask_seg, axis=0)\n",
    "    contig_final = contig[seg_mask][keep_mask_seg]\n",
    "    pos_final = pos[seg_mask][keep_mask_seg]\n",
    "    filter_pass_final = filter_pass[seg_mask][keep_mask_seg]\n",
    "    ref_final = ref[seg_mask][keep_mask_seg]\n",
    "    alt_final = alt[seg_mask][keep_mask_seg]\n",
    "    ann_final = ann[seg_mask][keep_mask_seg]\n",
    "\n",
    "    vcf_df = pd.DataFrame(\n",
    "        {\n",
    "            'CHROM': contig_final,\n",
    "            'POS': pos_final,\n",
    "            'FILTER_PASS': filter_pass_final,\n",
    "            'REF': ref_final,\n",
    "            'ALT': alt_final,\n",
    "            'ANN': ann_final,\n",
    "        }\n",
    "    )\n",
    "    geno_df = pd.DataFrame(geno_final.to_gt().astype(str), columns=samples)\n",
    "    vcf = pd.concat([vcf_df, geno_df], axis=1)\n",
    "\n",
    "    print(f\"Final DataFrame shape after initial filtering: {vcf.shape}\")\n",
    "\n",
    "    return vcf\n",
    "\n",
    "\n",
    "def apply_final_snp_filters(vcf_df, samples, max_missing_filter=0.20, min_maf_filter=0.02, genotypes_are_alt_counts=True):\n",
    "    \"\"\"Apply post-split/post-encoding SNP-level missingness and MAF filters.\"\"\"\n",
    "    geno_df = vcf_df[samples].copy()\n",
    "\n",
    "    if not genotypes_are_alt_counts:\n",
    "        geno_df = geno_df.applymap(\n",
    "            lambda gt: np.nan if pd.isna(gt) or gt == './.' else sum(allele != '0' for allele in str(gt).split('/'))\n",
    "        )\n",
    "\n",
    "    geno_df = geno_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    missing_frac = geno_df.isna().mean(axis=1)\n",
    "    miss_mask = missing_frac <= max_missing_filter\n",
    "\n",
    "    called_n = geno_df.notna().sum(axis=1)\n",
    "    alt_count = geno_df.sum(axis=1, skipna=True)\n",
    "    denom = 2 * called_n\n",
    "\n",
    "    alt_freq = pd.Series(np.nan, index=geno_df.index, dtype=float)\n",
    "    valid = denom > 0\n",
    "    alt_freq.loc[valid] = alt_count.loc[valid] / denom.loc[valid]\n",
    "\n",
    "    maf = pd.Series(np.nan, index=geno_df.index, dtype=float)\n",
    "    maf.loc[valid] = np.minimum(alt_freq.loc[valid], 1 - alt_freq.loc[valid])\n",
    "    maf_mask = maf > min_maf_filter\n",
    "\n",
    "    _print_filter_stats(\n",
    "        stage_label='final post-split',\n",
    "        total_snps=len(vcf_df),\n",
    "        miss_mask=miss_mask,\n",
    "        maf_mask=maf_mask,\n",
    "        max_missing_filter=max_missing_filter,\n",
    "        min_maf_filter=min_maf_filter,\n",
    "        extra_removed=None,\n",
    "    )\n",
    "\n",
    "    final_mask = miss_mask & maf_mask\n",
    "    return vcf_df.loc[final_mask].copy()\n",
    "\n",
    "\n",
    "def split_rows_with_multiple_alleles(df, samples):\n",
    "    # Create an empty list to store the new rows\n",
    "    new_rows = []\n",
    "    # Iterate through each row\n",
    "    for index, row in df.iterrows():\n",
    "        alt_alleles = row['ALT'].split(',')\n",
    "        # Check if there are multiple alleles in the ALT field\n",
    "        if len(alt_alleles) > 1:\n",
    "            for allele_num, allele in enumerate(alt_alleles):\n",
    "                # Create a new row for each allele\n",
    "                new_row = row.copy()\n",
    "                new_row['ALT'] = allele\n",
    "                # Update genotype fields\n",
    "                for col in samples:\n",
    "                    genotype = row[col]\n",
    "                    # Split the genotype and process it\n",
    "                    if genotype != './.':\n",
    "                        gt_alleles = genotype.split('/')\n",
    "                        new_gt = ['0' if (int(gt) != allele_num + 1 and gt != '0') else gt for gt in gt_alleles]\n",
    "                        new_row[col] = '/'.join(new_gt)\n",
    "                new_rows.append(new_row)\n",
    "        else:\n",
    "            new_rows.append(row)\n",
    "\n",
    "    new_df = pd.DataFrame(new_rows).reset_index(drop=True)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def convert_genotype_to_alt_allele_count(df, samples):\n",
    "    \"\"\"\n",
    "    Convert genotype data to alternate allele counts (0, 1, or 2).\n",
    "\n",
    "    Parameters:\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing genotype data, with genotypes in the format '0/0', '0/1', etc.\n",
    "    samples : list\n",
    "        List of sample IDs corresponding to genotype columns in the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame\n",
    "        A DataFrame with genotypes converted to alternate allele counts.\n",
    "    \"\"\"\n",
    "    nalt_df = df.copy()\n",
    "    # Iterate through each row\n",
    "    for index, row in df.iterrows():\n",
    "        # Update genotype fields\n",
    "        for col in samples:\n",
    "            genotype = row[col]\n",
    "            if genotype != './.':\n",
    "                # Split the genotype and count non-zero alleles\n",
    "                alleles = genotype.split('/')\n",
    "                alt_allele_count = sum([1 for allele in alleles if allele != '0'])\n",
    "                nalt_df.at[index, col] = alt_allele_count\n",
    "            else:\n",
    "                nalt_df.at[index, col] = np.nan\n",
    "\n",
    "    return nalt_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code -  run GLM and process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pseudo_r2(results):\n",
    "    \"\"\"\n",
    "    Calculate different pseudo R² measures for a fitted GLM model\n",
    "    \n",
    "    Parameters:\n",
    "    results : statsmodels GLMResults object\n",
    "        The fitted model results\n",
    "        \n",
    "    Returns:\n",
    "    dict : Dictionary containing different pseudo R² measures\n",
    "    \"\"\"\n",
    "    ll_null = results.null_deviance / -2\n",
    "    ll_model = results.deviance / -2\n",
    "    n = results.nobs\n",
    "    \n",
    "    r2_mcfadden = 1 - (ll_model / ll_null)\n",
    "    r2_coxsnell = 1 - np.exp((2/n) * (ll_null - ll_model))\n",
    "    r2_nagelkerke = r2_coxsnell / (1 - np.exp((2/n) * ll_null))\n",
    "    \n",
    "    return {\n",
    "        'McFadden R²': r2_mcfadden,\n",
    "        'Nagelkerke R²': r2_nagelkerke\n",
    "    }\n",
    "\n",
    "def glm_all_snps(snp_df):\n",
    "    import statsmodels.formula.api as smf\n",
    "    import statsmodels.api as sm\n",
    "    from statsmodels.stats.multitest import fdrcorrection\n",
    "    \n",
    "    dfs = []\n",
    "    for snp in snp_df.filter(like=\"snp_\").columns:\n",
    "        glm_data = snp_df[['phenotype', snp]].dropna()\n",
    "        glm_data[snp] = glm_data[snp].astype(int)\n",
    "        glm_data.columns = ['phenotype', 'snp']\n",
    "    \n",
    "        # df_contingency = pd.crosstab(glm_data['phenotype'], glm_data['snp'])\n",
    "        \n",
    "        formula = f'phenotype ~ snp'\n",
    "        logit_model = smf.glm(formula=formula, data=glm_data, family=sm.families.Binomial())\n",
    "        log_results = logit_model.fit()\n",
    "        \n",
    "        # Calculate pseudo R² measures\n",
    "        r2_values = calculate_pseudo_r2(log_results)\n",
    "    \n",
    "        pval_data = results_summary_to_dataframe(log_results)\n",
    "        # Add R² values to the results\n",
    "        for r2_name, r2_value in r2_values.items():\n",
    "            pval_data[r2_name] = r2_value\n",
    "            \n",
    "        dfs.append(pval_data.assign(snp=snp))\n",
    "        \n",
    "    df_eff = pd.concat(dfs).query(\"index != 'Intercept'\")\n",
    "    res = fdrcorrection(df_eff['pvals'], alpha=0.05)\n",
    "    df_eff['fdr'] = res[1]\n",
    "    df_eff['fdr_sig'] = res[0]\n",
    "    return process_effect_sizes(df_eff)\n",
    "\n",
    "\n",
    "def process_effect_sizes(df_eff):\n",
    "    \"\"\"Validate, annotate, and index per-SNP GLM effect sizes for downstream LD pruning/PGS.\"\"\"\n",
    "    # We expect one modeled row per SNP at this stage.\n",
    "    assert 'snp' in df_eff.columns, \"Expected 'snp' column in effect size dataframe.\"\n",
    "    assert df_eff['snp'].is_unique, \"Expected one GLM result per SNP; found duplicate SNP IDs before annotation merge.\"\n",
    "\n",
    "    # Parse SNP IDs (e.g. snp_2L:12345_A>G) back into merge keys.\n",
    "    parsed = df_eff['snp'].str.extract(r\"^snp_(?P<CHROM>[^:]+):(?P<POS>\\d+)_(?P<REF>[^>]+)>(?P<ALT>.+)$\")\n",
    "    assert parsed.notna().all().all(), (\n",
    "        \"Unable to parse one or more SNP IDs. Expected format: snp_<CHROM>:<POS>_<REF>><ALT>.\"\n",
    "    )\n",
    "\n",
    "    # Attach parsed keys to effect-size rows.\n",
    "    df_eff = pd.concat([df_eff, parsed], axis=1)\n",
    "    df_eff['POS'] = df_eff['POS'].astype(int)\n",
    "\n",
    "    # Ensure the annotation table has the columns needed to reconstruct SNP IDs.\n",
    "    required_ann_cols = {'CHROM', 'POS', 'REF', 'ALT', 'ANN'}\n",
    "    missing_ann_cols = required_ann_cols.difference(snp_data.columns)\n",
    "    assert not missing_ann_cols, f\"snp_data is missing required columns: {sorted(missing_ann_cols)}\"\n",
    "\n",
    "    # Build annotation SNP IDs using the same convention as GLM results.\n",
    "    ann_df = snp_data[list(required_ann_cols)].copy()\n",
    "    ann_df = ann_df.assign(\n",
    "        snp=lambda x: \"snp_\" + x.CHROM.astype(str) + \":\" + x.POS.astype(int).astype(str) + \"_\" + x.REF.astype(str) + \">\" + x.ALT.astype(str)\n",
    "    )\n",
    "\n",
    "    # Guard against ambiguous annotation lookups.\n",
    "    duplicate_ann = ann_df['snp'].duplicated(keep=False)\n",
    "    assert not duplicate_ann.any(), (\n",
    "        \"Annotation table contains duplicate SNP IDs even after using CHROM+POS+REF+ALT. \"\n",
    "        f\"Example duplicates: {ann_df.loc[duplicate_ann, 'snp'].head(5).tolist()}\"\n",
    "    )\n",
    "\n",
    "    # Merge ANN onto effect sizes and verify it is strictly one-to-one.\n",
    "    pre_merge_rows = len(df_eff)\n",
    "    df_eff = df_eff.merge(\n",
    "        ann_df[['snp', 'ANN']],\n",
    "        on='snp',\n",
    "        how='left',\n",
    "        validate='one_to_one'\n",
    "    )\n",
    "    assert len(df_eff) == pre_merge_rows, \"Annotation merge changed row count unexpectedly.\"\n",
    "\n",
    "    # Fail fast if any scored SNP lacks annotation.\n",
    "    missing_ann = df_eff['ANN'].isna()\n",
    "    assert not missing_ann.any(), (\n",
    "        \"Missing annotation for one or more SNPs after merge. \"\n",
    "        f\"Example SNPs: {df_eff.loc[missing_ann, 'snp'].head(5).tolist()}\"\n",
    "    )\n",
    "\n",
    "    # Final shape for downstream functions: SNP ID index + effect/annotation columns.\n",
    "    cols = ['snp'] + [col for col in df_eff.columns if col != 'snp']\n",
    "    df_eff = df_eff[cols].drop(columns=['CHROM', 'POS', 'REF', 'ALT'])\n",
    "    df_eff = df_eff.set_index('snp')\n",
    "    assert df_eff.index.is_unique, \"Effect size index must be unique before LD pruning.\"\n",
    "    return df_eff\n",
    "\n",
    "\n",
    "def results_summary_to_dataframe(results):\n",
    "    '''take the result of an statsmodel results table and transforms it into a dataframe'''\n",
    "    pvals = results.pvalues\n",
    "    coeff = results.params\n",
    "    conf_lower = results.conf_int()[0]\n",
    "    conf_higher = results.conf_int()[1]\n",
    "\n",
    "    results_df = pd.DataFrame({\"pvals\":pvals,\n",
    "                               \"odds_ratio\":np.exp(coeff),\n",
    "                               \"conf_lower\":np.exp(conf_lower),\n",
    "                               \"conf_higher\":np.exp(conf_higher)\n",
    "                                })\n",
    "    \n",
    "    results_df.loc[:, 'sig'] = [True if pval <= 0.05 else False for pval in pvals]\n",
    "    results_df = results_df[[\"odds_ratio\",\"pvals\",\"conf_lower\",\"conf_higher\", \"sig\"]]\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amp_samples = pd.read_csv(\"../../../results/config/metadata.qcpass.tsv\", sep=\"\\t\")\n",
    "df_amp_samples = df_amp_samples.assign(phenotype=df_amp_samples.sample_id.str.extract(r'_(Dead|Alive)_'))\n",
    "df_amp_samples = df_amp_samples.rename(columns={'sampleID':'sample_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_amp_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform association tests with separate binomial GLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load snp dataframe for snp annotations \n",
    "snp_data = pd.read_excel(\"../../../results/vcf/amplicons/natcomms-snps.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning, module=\"pandas.core.arraylike\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"allel.io\")\n",
    "\n",
    "df_genos = {}\n",
    "df_effs = {}\n",
    "# methods = ['targets', 'amplicons']\n",
    "methods  =  ['amplicons']\n",
    "for method in methods:\n",
    "    df_genos[method] = vcf_to_glm_data(\n",
    "        vcf_path=f\"../../../results/vcfs/{method}/natcomms.annot.vcf\", \n",
    "        df_samples=df_amp_samples, \n",
    "        sample_query='sample_id.str.contains(\"Siaya\")',\n",
    "        min_maf_filter=0.02, \n",
    "        max_missing_filter=0.20\n",
    "    )\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # run a glm on each snp on the input data \n",
    "    df_effs[method] = glm_all_snps(df_genos[method])\n",
    "    df_effs[method].to_csv(f\"../../../results/glm-siaya-effect-size-{method}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
