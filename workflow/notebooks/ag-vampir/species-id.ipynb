{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c0f6cb",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import allel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go  # type: ignore\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391f196",
   "metadata": {
    "tags": [
     "parameters",
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "metadata_path = '../../../results/config/metadata.qcpass.tsv'\n",
    "bed_targets_path = \"../../../config/ag-vampir.bed\"\n",
    "vcf_path = \"../../..//results/vcfs/targets/lab-strains.annot.vcf\"\n",
    "wkdir = \"../../..\"\n",
    "cohort_cols = 'location,taxon'\n",
    "platform = 'illumina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca2d73",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(wkdir, 'workflow'))\n",
    "import ampseekertools as amp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf306c",
   "metadata": {},
   "source": [
    "### Taxon classification\n",
    "\n",
    "In this analysis, we use three separate methods (AIMs, PCA, XGboost) to assign taxon to amplicon samples. For the latter two methods, we integrate data from over 7000 public accessible WGS samples from the [Vector Observatory](https://www.malariagen.net/vobs/).\n",
    "\n",
    "Accurate species identification is essential for mosquito surveillance as different members of the Anopheles gambiae complex have distinct ecological niches and vectorial capacities (Coetzee et al., 2013).\n",
    "\n",
    "#### Ancestry informative marker heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b7e45b",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "cohort_col = cohort_cols.split(',')[0]\n",
    "\n",
    "metadata = pd.read_csv(metadata_path , sep=\"\\t\")\n",
    "\n",
    "import json\n",
    "with open(f\"{wkdir}/results/config/metadata_colours.json\", 'r') as f:\n",
    "    color_mapping = json.load(f)\n",
    "    \n",
    "targets = pd.read_csv(bed_targets_path, sep=\"\\t\", header=None)\n",
    "targets.columns = ['contig', 'start', 'end', 'amplicon', 'mutation', 'ref', 'alt']\n",
    "\n",
    "gn_amp, pos, gn_contigs, metadata, refs, alts_amp, ann = amp.load_vcf(vcf_path=vcf_path, metadata=metadata, platform=platform)\n",
    "samples = metadata['sample_id'].to_list()\n",
    "\n",
    "alts_amp = np.concatenate([refs.reshape(refs.shape[0], -1), alts_amp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffdd883",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def _aims_n_alt(gt, aim_alts, data_alts):\n",
    "    n_sites = gt.shape[0]\n",
    "    n_samples = gt.shape[1]\n",
    "    # create empty array\n",
    "    aim_n_alt = np.empty((n_sites, n_samples), dtype=np.int8)\n",
    "\n",
    "    # for every site\n",
    "    for i in range(n_sites):\n",
    "        # find the index of the correct tag snp allele\n",
    "        tagsnp_index = np.where(aim_alts[i] == data_alts[i])[0]\n",
    "        for j in range(n_samples):\n",
    "            if tagsnp_index.shape[0] == 0:\n",
    "                aim_n_alt[i, j] = 0\n",
    "                continue\n",
    "\n",
    "            n_tag_alleles = np.sum(gt[i, j] == tagsnp_index[0])\n",
    "            n_missing = np.sum(gt[i, j] == -1)\n",
    "            if n_missing != 0:\n",
    "                aim_n_alt[i,j] = -1\n",
    "            else:\n",
    "                aim_n_alt[i, j] = n_tag_alleles\n",
    "\n",
    "    return aim_n_alt\n",
    "\n",
    "contigs = ['2R', '2L', '3R', '3L', 'X']\n",
    "df_aims = targets.query(\"mutation.str.contains('AIM')\", engine='python')\n",
    "\n",
    "aim_mask = np.isin(pos, df_aims.end.to_list())\n",
    "aim_gn = gn_amp.compress(aim_mask, axis=0)\n",
    "aim_pos = pos[aim_mask]\n",
    "aim_contigs = gn_contigs[aim_mask]\n",
    "aim_alts = alts_amp[aim_mask]\n",
    "\n",
    "aim_loc = [\"aim_\" + c + \":\" + str(aim_pos[i]) for i, c in enumerate(aim_contigs)]\n",
    "df_aims = df_aims.assign(loc=lambda x: \"aim_\" + x.contig + \":\" + x.end.astype(str)).set_index('loc')\n",
    "df_aims = df_aims.loc[aim_loc]\n",
    "\n",
    "aim_gn_alt = _aims_n_alt(aim_gn, aim_alts=df_aims.alt.to_list(), data_alts=aim_alts)\n",
    "df_aims = pd.concat([df_aims, pd.DataFrame(aim_gn_alt, columns=samples, index=aim_loc)], axis=1)\n",
    "df_aims = pd.concat([df_aims.query(f\"contig == '{contig}'\") for contig in contigs])\n",
    "\n",
    "# sort by cohort_col and then within that, by aim fraction \n",
    "aimplot_sample_order = []\n",
    "for coh in metadata[cohort_col].unique():\n",
    "    coh_samples = metadata.query(f\"{cohort_col} == '{coh}'\").sample_id.to_list()\n",
    "    coh_samples_aim_order = df_aims.iloc[:, 7:].loc[:, coh_samples].replace({-1: np.nan}).mean().sort_values(ascending=True).index.to_list()\n",
    "    aimplot_sample_order.extend(coh_samples_aim_order)\n",
    "\n",
    "# exclude samples with missing data\n",
    "# n_missing = df_aims.replace({-1: np.nan}).iloc[:, 7:].isna().sum(axis=0).sort_values(ascending=False)\n",
    "# missing_samples = n_missing[n_missing > 20].index.to_list()\n",
    "# aimplot_sample_order = [s for s in aimplot_sample_order if s not in missing_samples]\n",
    "from plotly.subplots import make_subplots\n",
    "col_widths = [\n",
    "    np.count_nonzero(aim_contigs == contig)\n",
    "    for contig in contigs\n",
    "]\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=len(contigs),\n",
    "    shared_yaxes=True,\n",
    "    column_titles=contigs,\n",
    "    row_titles=None,\n",
    "    column_widths=col_widths,\n",
    "    x_title=None,\n",
    "    y_title=None,\n",
    "    horizontal_spacing=0.01,\n",
    "    vertical_spacing=0.01,\n",
    ")\n",
    "\n",
    "species = \"gamb_vs_colu\".split(\"_vs_\")\n",
    "# Define a colorbar.\n",
    "colorbar = dict(\n",
    "    title=\"AIM genotype\",\n",
    "    tickmode=\"array\",\n",
    "    tickvals=[-1, 0, 1, 2],\n",
    "    ticktext=[\n",
    "        \"missing\",\n",
    "        f\"{species[0]}/{species[0]}\",\n",
    "        f\"{species[0]}/{species[1]}\",\n",
    "        f\"{species[1]}/{species[1]}\",\n",
    "    ],\n",
    "    len=100,\n",
    "    lenmode=\"pixels\",\n",
    "    y=1,\n",
    "    yanchor=\"top\",\n",
    "    outlinewidth=1,\n",
    "    outlinecolor=\"black\",\n",
    ")\n",
    "\n",
    "# Set up default AIMs color palettes.\n",
    "colors = px.colors.qualitative.T10\n",
    "color_gambcolu = colors[6]\n",
    "color_gamb = colors[0]\n",
    "color_gamb_colu_het = colors[5]\n",
    "color_colu = colors[2]\n",
    "color_missing = \"white\"\n",
    "palette = (\n",
    "        color_missing,\n",
    "        color_gamb,\n",
    "        color_gamb_colu_het,\n",
    "        color_colu,\n",
    "    )\n",
    "\n",
    "colorscale = [\n",
    "    [0 / 4, palette[0]],\n",
    "    [1 / 4, palette[0]],\n",
    "    [1 / 4, palette[1]],\n",
    "    [2 / 4, palette[1]],\n",
    "    [2 / 4, palette[2]],\n",
    "    [3 / 4, palette[2]],\n",
    "    [3 / 4, palette[3]],\n",
    "    [4 / 4, palette[3]],\n",
    "]\n",
    "\n",
    "# Create the subplots, one for each contig.\n",
    "for j, contig in enumerate(contigs):\n",
    "\n",
    "    df_aims_contig = df_aims.filter(like=contig, axis=0)\n",
    "    df_aims_contig = df_aims_contig.iloc[:, 7:]  \n",
    "    df_aims_contig = df_aims_contig.loc[:, aimplot_sample_order]\n",
    "    df_aims_contig = df_aims_contig.T\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            y=df_aims_contig.index,\n",
    "            z=df_aims_contig,\n",
    "            x=df_aims_contig.columns,\n",
    "            colorscale=colorscale,\n",
    "            zmin=-1.5,\n",
    "            zmax=2.5,\n",
    "            xgap=0,\n",
    "            ygap=0.5,  # this creates faint lines between rows\n",
    "            colorbar=colorbar,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=j + 1,\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"AIMs - gambiae vs coluzzii\",\n",
    "    height=max(600, 1.2 * len(samples) + 300),\n",
    "    width=800,\n",
    ")\n",
    "fig.write_image(f\"{wkdir}/results/aims_gamb_vs_colu.png\", scale=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21a22d",
   "metadata": {},
   "source": [
    "#### By cohort\n",
    "\n",
    "Ancestry Informative Markers (AIMs) are genomic variants with large allele frequency differences between populations, making them useful for species identification and detecting hybridization (Rosenberg et al., 2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe61122",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "df = df_aims.iloc[:, 7:]\n",
    "df.columns = samples\n",
    "\n",
    "## Use only chrom 3 and X aims for taxon \n",
    "x_3_mask = df.index.str.contains(\"aim_3|aim_X\")\n",
    "df = df[x_3_mask]\n",
    "\n",
    "mean_aims = df.replace(-1, float('nan')).apply(np.nanmean, axis=0)\n",
    "max_missing_aims = 12\n",
    "mean_aims[df.replace(-1, float('nan')).isna().sum(axis=0) > max_missing_aims] = np.nan\n",
    "aims = mean_aims.loc[metadata.set_index('sample_id').index]\n",
    "metadata = metadata.assign(mean_aim_genotype=aims.values)\n",
    "\n",
    "taxon = []\n",
    "for i, row in metadata.iterrows():\n",
    "    if row.mean_aim_genotype == np.nan:\n",
    "        taxon.append('uncertain')\n",
    "    elif row.mean_aim_genotype < 0.5:\n",
    "        taxon.append('gambiae')\n",
    "    elif row.mean_aim_genotype >= 0.5 and row.mean_aim_genotype < 1.5:\n",
    "        taxon.append('unassigned')\n",
    "    elif row.mean_aim_genotype >= 1.5:\n",
    "        taxon.append('coluzzii')\n",
    "    else: \n",
    "        taxon.append(np.nan)\n",
    "\n",
    "new_metadata = metadata.assign(aim_taxon=taxon)\n",
    "\n",
    "fig = px.histogram(\n",
    "    new_metadata,\n",
    "    nbins=100, \n",
    "    x='mean_aim_genotype', \n",
    "    color=cohort_col, \n",
    "    color_discrete_map=color_mapping[cohort_col],\n",
    "    width=750, \n",
    "    height=400, \n",
    "    template='plotly_white', \n",
    "    title='AIM genotype distribution'\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c00ff",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "new_metadata[['sample_id', 'aim_taxon', 'mean_aim_genotype']].to_csv(f\"{wkdir}/results/ag-vampir/aims/taxon_aims.tsv\", sep=\"\\t\", index=False)\n",
    "display(Markdown(f'<a href={wkdir}/results/ag-vampir/aims/taxon_aims.tsv>Sample gambiae vs coluzzii AIMs and AIM taxon assignment (.tsv)</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5050df-6692-4ab2-8401-4dc907e59a1c",
   "metadata": {},
   "source": [
    "### PCA-based taxon classification\n",
    "\n",
    "In this method, we use perform PCA and then apply a classifier to predict taxon based on VObs reference data. \n",
    "\n",
    "Principal Component Analysis reduces the dimensionality of genetic data while preserving patterns that differentiate species, enabling classification through machine learning algorithms (Jombart et al., 2010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd704333-b11c-455b-ac33-2fca523a89a8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "### Load VObs PCA data\n",
    "ampseq_loci = np.load(f\"{wkdir}/resources/ag-vampir/agvampir-ampseq-loci.npy\")\n",
    "alts_wgs = np.load(f\"{wkdir}/resources/ag-vampir/agvampir-alts-wgs.npy\")\n",
    "gn_wgs = np.load(f\"{wkdir}//resources/ag-vampir/agvampir-gn-wgs.npy\")\n",
    "\n",
    "df_wgs_samples = pd.read_csv(f\"{wkdir}/resources/ag-vampir/agvampir-df-samples.csv\", index_col=0)\n",
    "df_wgs_samples.shape\n",
    "    \n",
    "targets = pd.read_csv(bed_targets_path, sep=\"\\t\", header=None)\n",
    "targets.columns = ['contig', 'start', 'end', 'amplicon', 'mutation', 'ref', 'alt']\n",
    "\n",
    "gn_amp, pos, gn_contigs, metadata, refs, alts_amp, ann = amp.load_vcf(vcf_path=vcf_path, metadata=new_metadata, platform=platform)\n",
    "samples = metadata['sample_id'].to_list()\n",
    "\n",
    "alts = np.concatenate([refs.reshape(refs.shape[0], -1), alts_amp], axis=1)\n",
    "\n",
    "wgs_pos  = np.array([int(s.split(\":\")[1].split(\"-\")[0]) for s in ampseq_loci])\n",
    "\n",
    "wgs_dict = {value: idx for idx, value in enumerate(wgs_pos)}\n",
    "indices = [wgs_dict.get(value, -1) for value in pos]\n",
    "valid_indices = [idx for idx in indices if idx != -1]\n",
    "wgs_pos = np.array([wgs_pos[i] for i in valid_indices])\n",
    "gn_wgs = gn_wgs.take(valid_indices, axis=0)\n",
    "alts_wgs = alts_wgs.take(valid_indices, axis=0)\n",
    "\n",
    "assert all(np.array(wgs_pos) == pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de4157-a2a0-4680-ac51-3e12f7c869d4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import allel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def pca_all_samples(gn_wgs, gn_amp, df_wgs_samples, df_amp_samples, n_components=6):\n",
    "    \"\"\"\n",
    "    Perform PCA on both WGS and amplicon samples combined\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gn_wgs : GenotypeArray\n",
    "        Genotypes of samples with known taxon\n",
    "    gn_amp : GenotypeArray\n",
    "        Genotypes of samples to be assigned\n",
    "    df_wgs_samples : DataFrame\n",
    "        Metadata for samples with known taxon, including 'taxon' column\n",
    "    n_components : int\n",
    "        Number of principal components to compute\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pca_df : DataFrame\n",
    "        DataFrame containing PCA coordinates and metadata for all samples\n",
    "    \"\"\"\n",
    "    # Create metadata for amplicon samples\n",
    "    df_amp_samples = df_amp_samples.assign(sample_type='amplicon')\n",
    "    \n",
    "    # Add sample type to WGS metadata\n",
    "    df_wgs_samples = df_wgs_samples.copy()\n",
    "    df_wgs_samples = df_wgs_samples.assign(sample_type='WGS')\n",
    "    # Combine metadata\n",
    "    df_all_samples = pd.concat([df_wgs_samples.reset_index(), df_amp_samples[['sample_id', 'sample_type'] + cohort_cols.split(\",\")]], ignore_index=True)    \n",
    "    \n",
    "    # Combine genotype data\n",
    "    print(f\"Combining {gn_wgs.shape[1]} WGS samples and {gn_amp.shape[1]} amplicon samples\")\n",
    "    combined_geno = allel.GenotypeArray(np.concatenate([gn_wgs[:], gn_amp[:]], axis=1))\n",
    "    \n",
    "    # Perform PCA\n",
    "    print(\"Performing PCA on combined data\")\n",
    "    gn_alt = combined_geno.to_n_alt()\n",
    "    print(\"Removing any invariant sites\")\n",
    "    loc_var = np.any(gn_alt != gn_alt[:, 0, np.newaxis], axis=1)\n",
    "    gn_var = np.compress(loc_var, gn_alt, axis=0)\n",
    "    \n",
    "    print(f\"Running PCA with {gn_var.shape[0]} variable sites and {gn_var.shape[1]} samples\")\n",
    "    coords, model = allel.pca(gn_var, n_components=n_components)\n",
    "    \n",
    "    # Flip axes back so PC1 is same orientation in each window\n",
    "    for i in range(n_components):\n",
    "        c = coords[:, i]\n",
    "        if np.abs(c.min()) > np.abs(c.max()):\n",
    "            coords[:, i] = c * -1\n",
    "    \n",
    "    # Create PCA DataFrame\n",
    "    pca_df = pd.DataFrame(coords)\n",
    "    pca_df.columns = [f\"PC{pc+1}\" for pc in range(n_components)]\n",
    "    pca_df = pd.concat([df_all_samples.reset_index(drop=True), pca_df], axis=1)\n",
    "    \n",
    "    print(\"PCA completed successfully\")\n",
    "    return pca_df\n",
    "\n",
    "def assign_taxa(pca_df, method='knn', n_neighbors=5, probability_threshold=0.8, **kwargs):\n",
    "    \"\"\"\n",
    "    Assign taxa to amplicon samples based on PC1-4 coordinates\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_df : DataFrame\n",
    "        DataFrame with PCA coordinates and metadata for all samples\n",
    "    method : str\n",
    "        Classification method to use ('knn' or 'svm')\n",
    "    n_neighbors : int\n",
    "        Number of neighbors to use for KNN classification (ignored if method='svm')\n",
    "    probability_threshold : float\n",
    "        Minimum probability required for taxon assignment\n",
    "    **kwargs : dict\n",
    "        Additional parameters for the classifier\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    assignment_df : DataFrame\n",
    "        DataFrame with taxon assignments for amplicon samples\n",
    "    \"\"\"\n",
    "    from sklearn.svm import SVC\n",
    "    \n",
    "    # Separate training (WGS) and test (amplicon) data\n",
    "    wgs_samples = pca_df[pca_df['sample_type'] == 'WGS']\n",
    "    amp_samples = pca_df[pca_df['sample_type'] == 'amplicon']\n",
    "    \n",
    "    print(f\"Training data: {len(wgs_samples)} WGS samples\")\n",
    "    print(f\"Test data: {len(amp_samples)} amplicon samples\")\n",
    "    \n",
    "    # Extract features for training (PC1-4)\n",
    "    pc_features = ['PC1', 'PC2', 'PC3', 'PC4']\n",
    "    X_train = wgs_samples[pc_features].values\n",
    "    y_train = wgs_samples['taxon'].values\n",
    "    \n",
    "    # Extract features for testing\n",
    "    X_test = amp_samples[pc_features].values\n",
    "    \n",
    "    # Initialize classifier based on method\n",
    "    if method.lower() == 'knn':\n",
    "        print(f\"Training KNN classifier with {n_neighbors} neighbors\")\n",
    "        classifier = KNeighborsClassifier(n_neighbors=n_neighbors, **kwargs)\n",
    "    elif method.lower() == 'svm':\n",
    "        print(\"Training SVM classifier\")\n",
    "        # Ensure probability=True for SVM to get prediction probabilities\n",
    "        svm_kwargs = {'probability': True}\n",
    "        svm_kwargs.update(kwargs)\n",
    "        classifier = SVC(**svm_kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported method: {method}. Choose 'knn' or 'svm'\")\n",
    "    \n",
    "    # Train classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict taxa\n",
    "    predictions = classifier.predict(X_test)\n",
    "    probabilities = classifier.predict_proba(X_test)\n",
    "    max_probs = np.max(probabilities, axis=1)\n",
    "    \n",
    "    # Create assignment DataFrame\n",
    "    assignment_df = amp_samples[['sample_id']].copy()\n",
    "    \n",
    "    # Apply probability threshold for assignment\n",
    "    assignment_df['predicted_taxon'] = ['unassigned'] * len(amp_samples)\n",
    "    assignment_df['probability'] = 0.0\n",
    "    assignment_df['classifier'] = method.lower()\n",
    "    \n",
    "    for i, (pred, prob) in enumerate(zip(predictions, max_probs)):\n",
    "        if prob >= probability_threshold:\n",
    "            assignment_df.loc[assignment_df.index[i], 'predicted_taxon'] = pred\n",
    "            assignment_df.loc[assignment_df.index[i], 'probability'] = prob\n",
    "        else:\n",
    "            # For low confidence assignments, still store the prediction but mark as unassigned\n",
    "            assignment_df.loc[assignment_df.index[i], 'low_confidence_prediction'] = pred\n",
    "            assignment_df.loc[assignment_df.index[i], 'low_confidence_probability'] = prob\n",
    "    \n",
    "    # Count assignments\n",
    "    assigned_count = sum(assignment_df['predicted_taxon'] != 'unassigned')\n",
    "    print(f\"Assigned {assigned_count} out of {len(amp_samples)} samples with confidence ≥ {probability_threshold}\")\n",
    "    \n",
    "    if assigned_count < len(amp_samples):\n",
    "        print(f\"{len(amp_samples) - assigned_count} samples were below the confidence threshold\")\n",
    "    \n",
    "    # Print summary of assignments\n",
    "    print(\"\\nTaxon assignment summary:\")\n",
    "    print(assignment_df['predicted_taxon'].value_counts())\n",
    "    \n",
    "    return assignment_df\n",
    "\n",
    "def plot_pca_3d_with_assignments(pca_df, assignment_df, title=\"PCA with Taxon Assignments\", \n",
    "                              height=800, width=800):\n",
    "    \"\"\"\n",
    "    Create a 3D plot of PCA results with taxon assignments\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pca_df : DataFrame\n",
    "        DataFrame with PCA coordinates and metadata\n",
    "    assignment_df : DataFrame\n",
    "        DataFrame with taxon assignments for amplicon samples\n",
    "    title : str\n",
    "        Plot title\n",
    "    height, width : int\n",
    "        Plot dimensions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Create a copy of the PCA DataFrame for visualization\n",
    "    plot_df = pca_df.copy()\n",
    "    \n",
    "    # Update amplicon samples with assigned taxa\n",
    "    for i, row in assignment_df.iterrows():\n",
    "        sample_id = row['sample_id']\n",
    "        if row['predicted_taxon'] != 'unassigned':\n",
    "            # Update the taxon for this sample\n",
    "            plot_df.loc[plot_df['sample_id'] == sample_id, 'display_taxon'] = row['predicted_taxon'] + \" (predicted)\"\n",
    "        else:\n",
    "            # Mark as unassigned\n",
    "            plot_df.loc[plot_df['sample_id'] == sample_id, 'display_taxon'] = 'unassigned'\n",
    "    \n",
    "    # For WGS samples, use the known taxon but add a \"reference\" label\n",
    "    plot_df.loc[plot_df['sample_type'] == 'WGS', 'display_taxon'] = plot_df.loc[plot_df['sample_type'] == 'WGS', 'taxon'] + \" (VObs reference)\"\n",
    "    \n",
    "    # Split the data by sample type for different marker styles\n",
    "    wgs_samples = plot_df[plot_df['sample_type'] == 'WGS']\n",
    "    amp_samples = plot_df[plot_df['sample_type'] == 'amplicon']\n",
    "    \n",
    "    # Create an empty figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add reference (WGS) samples - use circles\n",
    "    for taxon in wgs_samples['display_taxon'].unique():\n",
    "        subset = wgs_samples[wgs_samples['display_taxon'] == taxon]\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=subset['PC1'],\n",
    "            y=subset['PC2'],\n",
    "            z=subset['PC3'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                symbol='circle',\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            name=taxon,\n",
    "            hovertemplate=\"<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<br>PC3: %{z:.2f}<br>Sample Type: Reference<br>Taxon: \" + taxon.replace(\" (reference)\", \"\") + \"<extra></extra>\",\n",
    "            text=subset['sample_id']\n",
    "        ))\n",
    "    \n",
    "    # Add amplicon samples - use diamonds/cross for greater visibility\n",
    "    for taxon in amp_samples['display_taxon'].unique():\n",
    "        subset = amp_samples[amp_samples['display_taxon'] == taxon]\n",
    "        \n",
    "        # Skip unassigned for a moment\n",
    "        if taxon == 'unassigned':\n",
    "            continue\n",
    "        \n",
    "        # For assigned samples, display probability\n",
    "        probs = []\n",
    "        for sid in subset['sample_id']:\n",
    "            prob = assignment_df.loc[assignment_df['sample_id'] == sid, 'probability'].values[0]\n",
    "            probs.append(f\"{prob:.2f}\")\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=subset['PC1'],\n",
    "            y=subset['PC2'],\n",
    "            z=subset['PC3'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=7,\n",
    "                symbol='diamond',\n",
    "                opacity=0.9\n",
    "            ),\n",
    "            name=taxon,\n",
    "            hovertemplate=\"<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<br>PC3: %{z:.2f}<br>Sample Type: Amplicon<br>Assigned Taxon: \" + taxon.replace(\" (predicted)\", \"\") + \"<br>Probability: %{customdata}<extra></extra>\",\n",
    "            text=subset['sample_id'],\n",
    "            customdata=probs\n",
    "        ))\n",
    "    \n",
    "    # Add unassigned amplicon samples\n",
    "    unassigned = amp_samples[amp_samples['display_taxon'] == 'unassigned']\n",
    "    if len(unassigned) > 0:\n",
    "        # For unassigned, show low confidence prediction if available\n",
    "        hover_data = []\n",
    "        for sid in unassigned['sample_id']:\n",
    "            row = assignment_df.loc[assignment_df['sample_id'] == sid].iloc[0]\n",
    "            if 'low_confidence_prediction' in row and not pd.isna(row['low_confidence_prediction']):\n",
    "                hover_data.append(f\"{row['low_confidence_prediction']} ({row['low_confidence_probability']:.2f})\")\n",
    "            else:\n",
    "                hover_data.append(\"No prediction\")\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=unassigned['PC1'],\n",
    "            y=unassigned['PC2'],\n",
    "            z=unassigned['PC3'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=7,\n",
    "                symbol='x',\n",
    "                color='gray',\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            name='unassigned',\n",
    "            hovertemplate=\"<b>%{text}</b><br>PC1: %{x:.2f}<br>PC2: %{y:.2f}<br>PC3: %{z:.2f}<br>Sample Type: Amplicon<br>Status: Unassigned<br>Low confidence: %{customdata}<extra></extra>\",\n",
    "            text=unassigned['sample_id'],\n",
    "            customdata=hover_data\n",
    "        ))\n",
    "    \n",
    "    # Improve layout\n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        scene=dict(\n",
    "            xaxis_title='PC1',\n",
    "            yaxis_title='PC2',\n",
    "            zaxis_title='PC3'\n",
    "        ),\n",
    "        height=height,\n",
    "        width=width,\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        legend_title_text='Taxa'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "mapping = allel.create_allele_mapping(alts_wgs[:, 0], alt=alts_amp, alleles=alts_wgs)\n",
    "gn_amp_remap = gn_amp.map_alleles(mapping)\n",
    "gn_amp_remap.shape\n",
    "\n",
    "snp_miss_mask = gn_amp_remap.is_missing().sum(axis=1) > 40\n",
    "snp_miss_mask.sum()\n",
    "\n",
    "gn_amp_flt = gn_amp_remap.compress(~snp_miss_mask, axis=0)\n",
    "gn_wgs_flt = gn_wgs.compress(~snp_miss_mask, axis=0)\n",
    "\n",
    "mask = df_wgs_samples.eval(\"taxon not in ['unassigned', 'fontenillei', 'gcx4', 'quadriannulatus', 'merus', 'melas']\")\n",
    "df_wgs_samples_flt = df_wgs_samples[mask]\n",
    "gn_wgs_flt = gn_wgs_flt.compress(mask, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621fa321-86d3-466d-aaec-274c17326735",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "pca_df = pca_all_samples(gn_wgs_flt, gn_amp_flt, df_wgs_samples_flt, metadata, n_components=6)\n",
    "\n",
    "df_assignments = assign_taxa(pca_df, method=\"svm\", n_neighbors=5, \n",
    "                           probability_threshold=0.8)\n",
    "\n",
    "metadata = metadata.assign(pca_taxon=df_assignments.predicted_taxon.to_numpy())\n",
    "\n",
    "pca_3d = plot_pca_3d_with_assignments(\n",
    "        pca_df, \n",
    "        df_assignments, \n",
    "        title=\"3D PCA with Taxon Assignments\"\n",
    ")\n",
    "\n",
    "pca_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83364250-4fa4-40ef-a06b-b4f44acdd958",
   "metadata": {},
   "source": [
    "### Machine learning based taxon classification\n",
    "\n",
    "In this method, we use an XGBoost algorithm to predict species based on the VObs reference data. \n",
    "\n",
    "XGBoost is a gradient boosting framework that uses decision trees to identify the most informative features for classification, offering high accuracy for species assignment (Chen & Guestrin, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb49b7-b2ed-47d3-83c1-20b7c38efae1",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "@numba.jit(nopython=True)\n",
    "def _melt_gt_counts(gt_counts):\n",
    "    n_snps, n_samples, n_alleles = gt_counts.shape\n",
    "    # Use a float array to allow NaN values\n",
    "    melted_counts = np.full((n_snps * (n_alleles - 1), n_samples), np.nan, dtype=np.float64)\n",
    "\n",
    "    for i in range(n_snps):\n",
    "        for j in range(n_samples):\n",
    "            for k in range(n_alleles - 1):\n",
    "                # Check if the genotype count is valid (== 2)\n",
    "                if gt_counts[i][j].sum() == 2:\n",
    "                    melted_counts[(i * (n_alleles - 1)) + k][j] = gt_counts[i][j][k + 1]\n",
    "                else:\n",
    "                    # Assign NaN for missing or invalid data\n",
    "                    melted_counts[(i * (n_alleles - 1)) + k][j] = np.nan\n",
    "\n",
    "    return melted_counts\n",
    "\n",
    "import pickle\n",
    "import xgboost \n",
    "\n",
    "with open(f\"{wkdir}/resources/ag-vampir/xgboost_taxon.pickle\", 'rb') as file:\n",
    "        tree = pickle.load(file)\n",
    "taxon_tree_features = tree['feature_importance'].sort_index().SNP.to_list()\n",
    "taxon_tree_feat_pos = np.unique([int(t.split(\":\")[1].split(\"-\")[1]) for t in taxon_tree_features])\n",
    "\n",
    "aim_gn = gn_amp.copy()\n",
    "aim_pos = pos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f879c6eb-2688-47e5-80a1-e9225a89a5f8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "#All nucleotides that should be in each row\n",
    "all_nucleotides = set(['A', 'C', 'G', 'T'])\n",
    "\n",
    "arr = alts.copy()\n",
    "\n",
    "# Fill in the missing values\n",
    "for i in range(arr.shape[0]):\n",
    "    # Get the non-empty values in the current row\n",
    "    present = set([val for val in arr[i] if val != ''])\n",
    "    \n",
    "    # Find the missing nucleotides\n",
    "    missing = all_nucleotides - present\n",
    "    \n",
    "    # Fill in the missing values\n",
    "    j = 0\n",
    "    for k in range(arr.shape[1]):\n",
    "        if arr[i, k] == '':\n",
    "            if j < len(missing):\n",
    "                arr[i, k] = list(missing)[j]\n",
    "                j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e480d6d-1dfb-47af-94bb-b582bbe7ed3f",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "gn_counts = _melt_gt_counts(gn_amp.to_allele_counts(max_allele=3).values)\n",
    "\n",
    "alts_flat = arr[:, 1:].flatten()\n",
    "pos_flat = np.repeat(pos, 3)\n",
    "\n",
    "labels = np.array([f\"{np.repeat(gn_contigs, 3)[i]}:{pos_flat[i]}-{pos_flat[i]}:{alts_flat[i]}\" for i in range(len(alts_flat))])\n",
    "\n",
    "df_vampir = pd.DataFrame(gn_counts, index=labels, columns=samples)\n",
    "missing_features = [feature for feature in taxon_tree_features if feature not in df_vampir.index]\n",
    "if missing_features:\n",
    "    missing_df = pd.DataFrame(0, index=missing_features, columns=df_vampir.columns)\n",
    "    df_vampir = pd.concat([df_vampir, missing_df])\n",
    "    print(f\"WARNING: The following features were missing and artificially added with zeros: {missing_features}\")\n",
    "    print(\"Taxon assignment using XGBoost may not be accurate due to these missing features.\")\n",
    "\n",
    "df_vampir = df_vampir.loc[taxon_tree_features, :]\n",
    "\n",
    "taxon_labels = list(tree['per_taxon_results'].keys())\n",
    "pred = tree['model'].predict(df_vampir.T)\n",
    "metadata = metadata.assign(tree_taxon=tree['label_encoder'].inverse_transform(pred))\n",
    "\n",
    "pivot_table = metadata.pivot_table(\n",
    "    index=cohort_col,\n",
    "    columns='tree_taxon',\n",
    "    values='sample_id',  \n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6aa4d3",
   "metadata": {},
   "source": [
    "### Consensus taxon calls summary\n",
    "\n",
    "Consensus taxonomic assignment combines predictions from multiple methods, reducing errors and improving confidence in species identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8721a28-9877-45ee-8380-3cd50e9634b8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def get_consensus_taxon(row):\n",
    "    # Extract the taxon values\n",
    "    aim_taxon = row['aim_taxon']\n",
    "    pca_taxon = row['pca_taxon']\n",
    "    tree_taxon = row['tree_taxon']\n",
    "    \n",
    "    # Count occurrences of each taxon\n",
    "    taxon_counts = {}\n",
    "    for taxon in [aim_taxon, pca_taxon, tree_taxon]:\n",
    "        if pd.notna(taxon):  # Skip NaN values\n",
    "            taxon_counts[taxon] = taxon_counts.get(taxon, 0) + 1\n",
    "    \n",
    "    # Find the most common taxon\n",
    "    max_count = 0\n",
    "    max_taxon = None\n",
    "    for taxon, count in taxon_counts.items():\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_taxon = taxon\n",
    "    \n",
    "    # If at least two columns agree, return that taxon, otherwise 'unassigned'\n",
    "    if max_count >= 2:\n",
    "        return max_taxon\n",
    "    else:\n",
    "        return 'unassigned'\n",
    "\n",
    "# Apply the function to create the new column\n",
    "metadata['taxon'] = metadata.apply(get_consensus_taxon, axis=1)\n",
    "\n",
    "metadata.to_csv(f\"{wkdir}/results/config/metadata.qcpass.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "pivot_table = metadata.pivot_table(\n",
    "    index=cohort_col,\n",
    "    columns='taxon',\n",
    "    values='sample_id',  \n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa29f1-bf12-4f39-9cfd-e3314ce1b145",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "cohort_cols_header = cohort_cols.split(',')\n",
    "if 'taxon' in cohort_cols_header:\n",
    "    cohort_cols_header.remove('taxon')\n",
    "\n",
    "show_cols = ['sample_id'] + cohort_cols_header + ['mean_aim_genotype', 'aim_taxon', 'pca_taxon', 'tree_taxon', 'taxon']\n",
    "\n",
    "metadata[show_cols]"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
